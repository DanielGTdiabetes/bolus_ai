
import logging
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_

from app.models.suggestion import ParameterSuggestion
from app.models.evaluation import SuggestionEvaluation
from app.models.analysis import BolusPostAnalysis

logger = logging.getLogger(__name__)

async def evaluate_suggestion_service(
    suggestion_id: str,
    user_id: str,
    days: int,
    db: AsyncSession
) -> dict:
    # 1. Fetch Suggestion
    stmt = select(ParameterSuggestion).where(
        ParameterSuggestion.id == suggestion_id,
        ParameterSuggestion.user_id == user_id
    )
    res = await db.execute(stmt)
    sug = res.scalars().first()
    
    if not sug:
        return {"error": "Suggestion not found"}
        
    if sug.status != "accepted" or not sug.resolved_at:
        return {"error": "Suggestion not accepted yet"}
        
    # 2. Define Periods
    # Before: [resolved_at - days, resolved_at)
    # After: [resolved_at, resolved_at + days]
    
    # Actually, we should bound "After" to "Now" if days haven't passed? 
    # But usually we want comparable windows.
    # Let's use strict days.
    
    mid_point = sug.resolved_at
    start_before = mid_point - timedelta(days=days)
    end_after = mid_point + timedelta(days=days)
    
    # 3. Fetch Analysis Data
    # specific to meal_slot and window (from evidence)
    meal_slot = sug.meal_slot
    # Extract window from evidence: e.g. "3h" -> 3
    window_h = 3 # default
    if sug.evidence and "window" in sug.evidence:
        w_str = str(sug.evidence["window"]).replace("h", "")
        if w_str.isdigit():
            window_h = int(w_str)
            
    # Query Analysis
    q = select(BolusPostAnalysis).where(
        BolusPostAnalysis.user_id == user_id,
        BolusPostAnalysis.meal_slot == meal_slot,
        BolusPostAnalysis.window_h == window_h,
        BolusPostAnalysis.bolus_at >= start_before,
        BolusPostAnalysis.bolus_at <= end_after
    )
    
    rows_res = await db.execute(q)
    rows = rows_res.scalars().all()
    
    # 4. Buckets
    before_stats = {"n": 0, "short": 0, "ok": 0, "over": 0, "excluded": 0}
    after_stats = {"n": 0, "short": 0, "ok": 0, "over": 0, "excluded": 0}
    
    # Using timestamps to bucket
    # Note: timestamps in DB are timezone aware (UTC) usually
    
    for r in rows:
        # Exclude bad quality
        if r.iob_status == "unavailable":
            if r.bolus_at < mid_point:
                before_stats["excluded"] += 1
            else:
                after_stats["excluded"] += 1
            continue
            
        target_dict = before_stats if r.bolus_at < mid_point else after_stats
        
        target_dict["n"] += 1
        if r.result == "short":
            target_dict["short"] += 1
        elif r.result == "over":
            target_dict["over"] += 1
        elif r.result == "ok":
            target_dict["ok"] += 1
        # missing is ignored for N
        
    # 5. Score Calculation
    def calc_score(stats, metric):
        if stats["n"] < 5: return None
        if metric == "short":
            return stats["short"] / stats["n"]
        elif metric == "over":
            return stats["over"] / stats["n"]
        return max(stats["short"]/stats["n"], stats["over"]/stats["n"])

    # Determine primary metric from suggestion reason
    # If generated by our engine, we know the reason logic. 
    # Or check evidence ratios.
    # Simple heuristic: if short > over in "before", metric = short.
    
    metric = "generic"
    # We can infer from suggestion object but fields like 'direction' are Generic (Review).
    # Let's look at "before" distribution or evidence if available.
    # Fallback to evidence in suggestion
    if sug.evidence and "counts" in sug.evidence:
        c = sug.evidence["counts"]
        if c.get("short", 0) > c.get("over", 0):
            metric = "short"
        elif c.get("over", 0) > c.get("short", 0):
            metric = "over"
            
    score_before = calc_score(before_stats, metric)
    score_after = calc_score(after_stats, metric)
    
    evaluation_result = "insufficient"
    summary_text = "Datos insuficientes en alguno de los periodos."
    
    if score_before is not None and score_after is not None:
        delta = 0.15
        diff = score_after - score_before 
        # Aim is LOWER score (less errors)
        
        if diff <= -delta:
            evaluation_result = "improved"
            summary_text = f"Mejoró: redujiste fallos ({metric}) de {int(score_before*100)}% a {int(score_after*100)}%."
        elif diff >= delta:
            evaluation_result = "worse"
            summary_text = f"Empeoró: aumentaron fallos ({metric}) de {int(score_before*100)}% a {int(score_after*100)}%."
        else:
            evaluation_result = "no_change"
            summary_text = f"Sin cambios claros ({int(score_before*100)}% vs {int(score_after*100)}%)."
            
    # 6. Store Evaluation
    # Check if exists to update
    stmt_eval = select(SuggestionEvaluation).where(SuggestionEvaluation.suggestion_id == sug.id)
    existing_eval_res = await db.execute(stmt_eval)
    existing_eval = existing_eval_res.scalars().first()
    
    if existing_eval:
        evaluation = existing_eval
        evaluation.status = "evaluated"
        evaluation.result = evaluation_result
        evaluation.summary = summary_text
        evaluation.evidence = {
            "window_h": window_h,
            "days": days,
            "metric": metric,
            "before": {**before_stats, "score": score_before},
            "after": {**after_stats, "score": score_after}
        }
        evaluation.evaluated_at = datetime.utcnow()
    else:
        evaluation = SuggestionEvaluation(
            suggestion_id=sug.id,
            analysis_days=days,
            status="evaluated",
            result=evaluation_result,
            summary=summary_text,
            evidence={
                "window_h": window_h,
                "days": days,
                "metric": metric,
                "before": {**before_stats, "score": score_before},
                "after": {**after_stats, "score": score_after}
            },
            evaluated_at=datetime.utcnow()
        )
        db.add(evaluation)
        
    await db.commit()
    await db.refresh(evaluation)
    
    return evaluation

async def list_evaluations_service(user_id: str, db: AsyncSession):
    # Join Suggestion to filter by user and order
    # Or just select SuggestionEvaluations where suggestion.user_id = ...
    
    stmt = select(SuggestionEvaluation).join(ParameterSuggestion).where(
        ParameterSuggestion.user_id == user_id
    ).order_by(SuggestionEvaluation.created_at.desc())
    
    res = await db.execute(stmt)
    return res.scalars().all()
